{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from collections import namedtuple\n",
    "from multiprocessing import Pool\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMENSION = 10\n",
    "QUERY_COUNT = 1000\n",
    "ITEM_COUNT = 10 ** 6\n",
    "GT_TOP_LEN = 10000 # maximal value scince hnsw doesn't give more than 1000 neighbors\n",
    "RECALL_TOP_LEN = 5\n",
    "\n",
    "MIN_EF_SEARCH = 1\n",
    "MAX_EF_SEARCH = 10000\n",
    "HIDDEN_DIMENSIONS = list(range(4))\n",
    "RECALL_OF_INTEREST = 0.95\n",
    "N_SEARCH_THREADS = 8\n",
    "REGENERATE_DATA = False\n",
    "DATASET = \"synthetic\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_graph_cmd_template = (\n",
    "    \"./RPG --mode base \"\n",
    "    \"--baseSize 1000000 \"\n",
    "    \"--trainQueries {featuresSize} \"\n",
    "    \"--base data/synthetic/data/{features}.bin \"\n",
    "    \"--outputGraph {graphPath} \"\n",
    "    \"--relevanceVector {relevanceVector} \"\n",
    "    \"--efConstruction 1000 --M 8 \"\n",
    "    \"--metric {metric}\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_cmd_template = (\n",
    "    \"./RPG --mode query --baseSize 1000000 --querySize 1000\"\n",
    "    \" --query data/synthetic/data/model_scores/{scores} --inputGraph {inputGraph}\"\n",
    "    \" --efSearch {efSearch} --topK {topK} --output data/synthetic/{searchResultFile}.txt\" +\n",
    "    \" --gtQueries 1000 --gtTop {} \".format(GT_TOP_LEN) +\n",
    "    \"--groundtruth data/synthetic/data/model_scores/{gtFile}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(a):\n",
    "    vec_lengths = np.sqrt(np.power(a, 2).sum(axis=1, keepdims=True))\n",
    "    return a / vec_lengths\n",
    "\n",
    "def generate_or_read_data(file_name, shape):\n",
    "    if not os.path.isfile(file_name):\n",
    "        data = normalize(np.random.randn(*shape)).astype(\"float32\")\n",
    "        data.tofile(file_name)\n",
    "    else:\n",
    "        data = np.fromfile(file_name, dtype=\"float32\").reshape(shape)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by(results, key):\n",
    "    assert key in [\"evals\", \"efSearch\"]\n",
    "    permutation = np.argsort(results[key])\n",
    "    for key in results:\n",
    "        vals = results[key]\n",
    "        results[key] = [vals[i] for i in permutation]\n",
    "    return results\n",
    "\n",
    "assert sort_by({\n",
    "    \"efSearch\": [2, 0, 3, 1],\n",
    "    \"vals\": [1, 2, 3, 4]}, \"efSearch\") == {\n",
    "    \"efSearch\": [0, 1, 2, 3],\n",
    "    \"vals\": [2, 4, 1, 3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evals_for_recall(results, recall=RECALL_OF_INTEREST):\n",
    "    results = sort_by(results, \"evals\")\n",
    "    evals = results[\"evals\"]\n",
    "    recalls = results[\"recall\"]\n",
    "    ef_serch = results[\"efSearch\"] if \"efSearch\" in results else evals\n",
    "    assert len(evals) == len(recalls)\n",
    "    \n",
    "    lower_bound = 0\n",
    "    lower_bound_ef = MIN_EF_SEARCH\n",
    "    upper_bound = math.inf\n",
    "    upper_bound_ef = MAX_EF_SEARCH\n",
    "    \n",
    "    if not evals:\n",
    "        return lower_bound, upper_bound\n",
    "    \n",
    "    \n",
    "    if recalls[0] <= recall:\n",
    "        lower_bound = evals[0]\n",
    "        lower_bound_ef = ef_serch[0]\n",
    "        i = 1\n",
    "        while i < len(evals) and recalls[i] <= recall:\n",
    "            lower_bound = evals[i]\n",
    "            lower_bound_ef = ef_serch[i]\n",
    "            i += 1\n",
    "    \n",
    "    if recalls[-1] >= recall:\n",
    "        upper_bound = evals[-1]\n",
    "        upper_bound_ef = ef_serch[-1]\n",
    "        i = len(evals) - 2\n",
    "        while i >= 0 and recalls[i] >= recall:\n",
    "            upper_bound = evals[i]\n",
    "            upper_bound_ef = ef_serch[i]\n",
    "            i -= 1\n",
    "    \n",
    "    return (lower_bound, upper_bound), (lower_bound_ef, upper_bound_ef)\n",
    "\n",
    "assert get_evals_for_recall({\n",
    "    \"evals\": [0, 1, 2, 3, 4],\n",
    "    \"recall\": [0.93, 0.94, 0.95, 0.96, 0.97]\n",
    "}) == ((2, 2), (2, 2))\n",
    "assert get_evals_for_recall({\n",
    "    \"evals\": [0, 1, 2, 3, 4],\n",
    "    \"recall\": [0.93, 0.94, 0.955, 0.96, 0.97]\n",
    "}) == ((1, 2), (1, 2))\n",
    "assert get_evals_for_recall({\n",
    "    \"evals\": [0, 1, 2, 3, 4],\n",
    "    \"recall\": [0.93, 0.96, 0.94, 0.96, 0.97]\n",
    "}) == ((0, 3), (0, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_results(lhs_res, rhs_res):\n",
    "    assert not rhs_res or sorted(lhs_res.keys()) == sorted(rhs_res.keys())\n",
    "    merged_res = {}\n",
    "    for key in lhs_res:\n",
    "        merged_res[key] = list(lhs_res[key]) + list(rhs_res.get(key, []))\n",
    "    return merged_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logspace(start, stop, count, include_ends=True):\n",
    "    cnt_ = count if include_ends else count + 2\n",
    "    seq = np.unique(np.exp(\n",
    "        np.linspace(np.log(start), np.log(stop), cnt_)\n",
    "    ).astype(\"int\"))\n",
    "    if include_ends:\n",
    "        return seq\n",
    "    return seq[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bench_cmd(cmd):\n",
    "    res = {}\n",
    "    cmd_out = []\n",
    "    with os.popen(cmd) as out:\n",
    "        for line in out:\n",
    "            cmd_out.append(line)\n",
    "            for stat_name, prefix, suff_len in [\n",
    "                (\"relevance\", \"Average relevance: \", 1),\n",
    "                (\"recall\", \"Recall@5: \", 1),\n",
    "                (\"time\", \"Average query time: \", 3),\n",
    "                (\"evals\", \"Average number of model computations: \", 1)\n",
    "            ]:\n",
    "                if line.startswith(prefix):\n",
    "                    res[stat_name] = float(line[len(prefix):-suff_len])\n",
    "    return res, \"\".join(cmd_out)\n",
    "\n",
    "def run_search(graph_path, scores_file, ef_ticks, topK=5,\n",
    "               result_file=None, n_threads=8, verbose=True,\n",
    "               gt_file=\"groundtruth_test.bin\"\n",
    "              ):\n",
    "    if result_file is None:\n",
    "        result_file = \"result.txt\"\n",
    "    else:\n",
    "        assert len(ef_ticks) == 1\n",
    "    \n",
    "    commands = []\n",
    "    for ef in ef_ticks:\n",
    "        commands.append(search_cmd_template.format(\n",
    "            scores=scores_file,\n",
    "            inputGraph=graph_path,\n",
    "            efSearch=ef,\n",
    "            topK=topK,\n",
    "            searchResultFile=result_file,\n",
    "            gtFile=gt_file\n",
    "        ))\n",
    "    pool = Pool(processes=n_threads)\n",
    "    results = pool.map(bench_cmd, commands)\n",
    "    output = {\"relevance\": [], \"recall\": [], \"time\": [], \"evals\": []}\n",
    "    for i, (res, cmd_out) in enumerate(results):\n",
    "        if all(key in res for key in output):\n",
    "            for key in output:\n",
    "                output[key].append(res[key])\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"missed result for {} efSearch {}.\".format(graph_path, ef_ticks[i]))\n",
    "                print(cmd_out)\n",
    "    output[\"efSearch\"] = list(ef_ticks)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_recall(graph_path, scores_file,\n",
    "                  iterations=2, runs_per_iteration=None,\n",
    "                  recall_of_interest=RECALL_OF_INTEREST, n_threads=8,\n",
    "                  gt_file=\"groundtruth_test.bin\"\n",
    "                 ):\n",
    "    if runs_per_iteration is None:\n",
    "        runs_per_iteration = n_threads\n",
    "    \n",
    "    lower_ef_search_bound = MIN_EF_SEARCH\n",
    "    upper_ef_search_bound = MAX_EF_SEARCH\n",
    "    cur_results = {}\n",
    "    for i in range(iterations):\n",
    "        ef_ticks = logspace(lower_ef_search_bound, upper_ef_search_bound,\n",
    "                            runs_per_iteration, include_ends=(i==0))\n",
    "        result_update = run_search(graph_path, scores_file,\n",
    "                                   ef_ticks=ef_ticks, n_threads=n_threads, gt_file=gt_file)\n",
    "        cur_results = merge_results(result_update, cur_results)\n",
    "        _, (lower_ef_search_bound, upper_ef_search_bound) = get_evals_for_recall(\n",
    "            cur_results, recall=recall_of_interest)\n",
    "    return sort_by(cur_results, \"efSearch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(file_name, expected_shape=None):\n",
    "    data = []\n",
    "    with open(file_name) as fin:\n",
    "        for line in fin:\n",
    "            data.append([int(w) for w in line.split()])\n",
    "    row_len = len(data[0])\n",
    "    assert all(len(l) == row_len for l in data)\n",
    "    if expected_shape is not None:\n",
    "        assert expected_shape == (len(data), row_len)\n",
    "    return data\n",
    "\n",
    "\n",
    "def calc_eval_recall_curve(approximate_top, gt_top):\n",
    "    assert gt_top.shape == (QUERY_COUNT, RECALL_TOP_LEN)\n",
    "    \n",
    "    gt_tops = [set(query_top) for query_top in gt_top]\n",
    "    recalls = []\n",
    "    found_count = 0\n",
    "    top_len = len(approximate_top[0])\n",
    "    for i in range(top_len):\n",
    "        for query_id in range(QUERY_COUNT):\n",
    "            if approximate_top[query_id][i] in gt_tops[query_id]:\n",
    "                found_count += 1\n",
    "        recalls.append(found_count / (QUERY_COUNT * RECALL_TOP_LEN))\n",
    "    evals = list(range(1, top_len + 1))\n",
    "    return {\"evals\": evals, \"recall\": recalls}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test calc_eval_recall_curve\n",
    "\n",
    "# full_test_top = np.vstack([\n",
    "#     np.arange(100) for i in range(QUERY_COUNT)\n",
    "# ])\n",
    "# test_gt = full_test_top[:,:RECALL_TOP_LEN]\n",
    "# approx_top = full_test_top.copy()\n",
    "# for row in approx_top:\n",
    "#     np.random.shuffle(row[:80])\n",
    "# test_curve = calc_eval_recall_curve(approx_top, test_gt)\n",
    "# plt.plot(test_curve[\"evals\"], test_curve[\"recall\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dataset(items, queries, samples_per_query=1000):\n",
    "    assert items.shape == (ITEM_COUNT, DIMENSION)\n",
    "    assert queries.shape == (QUERY_COUNT, DIMENSION)\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    for i in range(samples_per_query):\n",
    "        item_indexes = np.random.choice(ITEM_COUNT, QUERY_COUNT, replace=False)\n",
    "        chosen_items = items[item_indexes]\n",
    "        x_batches.append(np.hstack((chosen_items, queries)))\n",
    "        y_batches.append(np.sum(chosen_items * queries, axis=1))\n",
    "    return np.vstack(x_batches), np.hstack(y_batches)\n",
    "\n",
    "# test sample dataset\n",
    "\n",
    "# X, y = sample_dataset(items, train_queries, 1)\n",
    "\n",
    "# assert X.shape == (QUERY_COUNT, 2 * DIMENSION)\n",
    "# assert y.shape == (QUERY_COUNT,)\n",
    "\n",
    "# dot_prod = 0\n",
    "# for i in range(DIMENSION):\n",
    "#     dot_prod += X[0][i] * X[0][DIMENSION + i]\n",
    "# assert np.allclose([dot_prod], [y[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(items, queries, model, out_file):\n",
    "    assert queries.shape == (QUERY_COUNT, DIMENSION)\n",
    "    assert items.shape == (ITEM_COUNT, DIMENSION)\n",
    "    per_item_predictions = []\n",
    "    for item in items:\n",
    "        ifeats = np.repeat(item, QUERY_COUNT).reshape((DIMENSION, QUERY_COUNT)).T\n",
    "        feats = np.hstack((ifeats, queries))\n",
    "        per_item_predictions.append(model.predict(feats).astype(\"float32\"))\n",
    "    np.vstack(per_item_predictions).tofile(out_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REGENERATE_DATA:\n",
    "    print(\"Generate synthetic queries and items\") \n",
    "    train_queries = normalize(np.random.randn(QUERY_COUNT, DIMENSION)).astype(\"float32\")\n",
    "    train_queries.tofile(\"data/synthetic/data/train_queries.bin\")\n",
    "\n",
    "    test_queries = normalize(np.random.randn(QUERY_COUNT, DIMENSION)).astype(\"float32\")\n",
    "    test_queries.tofile(\"data/synthetic/data/test_queries.bin\")\n",
    "    \n",
    "    items = normalize(np.random.randn(ITEM_COUNT, DIMENSION)).astype(\"float32\")\n",
    "    items.tofile(\"data/synthetic/data/items.bin\")\n",
    "\n",
    "    print(\"compute ground truth test scores\")\n",
    "    gt_train_scores = items.dot(train_queries.T)\n",
    "    gt_train_scores.tofile(\"data/synthetic/data/model_scores/gt_train_scores.bin\")\n",
    "    del gt_train_scores\n",
    "\n",
    "    gt_test_scores = items.dot(test_queries.T)\n",
    "    gt_test_scores.tofile(\"data/synthetic/data/model_scores/gt_test_scores.bin\")\n",
    "    del gt_test_scores\n",
    "    \n",
    "    print(\"compute test scores for models with hidden dimensions\") \n",
    "    for hidden_dim_count in HIDDEN_DIMENSIONS:\n",
    "        hidden_model_test_scores = items[:,:DIMENSION - hidden_dim_count].dot(\n",
    "            test_queries.T[:DIMENSION - hidden_dim_count]\n",
    "        )\n",
    "        hidden_model_test_scores.tofile(\n",
    "            \"data/synthetic/data/model_scores/hidden_{}_test_scores.bin\".format(\n",
    "                hidden_dim_count\n",
    "            )\n",
    "        )\n",
    "        del hidden_model_test_scores\n",
    "\n",
    "    print(\"Calc ground truth nearest neighbors\")\n",
    "    for data_part in [\"train\", \"test\"]:\n",
    "        scores_path = \"data/synthetic/data/model_scores/gt_{}_scores.bin\".format(data_part)\n",
    "        scores = np.fromfile(scores_path, dtype=\"float32\").reshape(\n",
    "            (ITEM_COUNT, QUERY_COUNT))\n",
    "        gt = (-scores).argsort(axis=0)[:GT_TOP_LEN,:].T.astype(\"int32\")\n",
    "        gt.tofile(\"data/synthetic/data/model_scores/groundtruth_{}.bin\".format(data_part))\n",
    "        del scores\n",
    "        del gt\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_search_gt_results = {}\n",
    "hidden_search_rerank_results = {}\n",
    "relevance_search_gt_results = {}\n",
    "\n",
    "hidden_search_gt_evals = []\n",
    "hidden_search_rerank_evals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for hidden_dim_count in HIDDEN_DIMENSIONS:\n",
    "    graph_path = \"data/synthetic/hidden_{}_of_{}_graph.out\".format(\n",
    "        hidden_dim_count, DIMENSION\n",
    "    )\n",
    "    if not os.path.isfile(graph_path) or REGENERATE_DATA:\n",
    "        build_cmd = build_graph_cmd_template.format(\n",
    "            featuresSize=DIMENSION,\n",
    "            graphPath=graph_path,\n",
    "            relevanceVector=DIMENSION - hidden_dim_count,\n",
    "            features=\"items\",\n",
    "            metric=\"dot_product\" # should be equivalent to l2\n",
    "        )\n",
    "        print(build_cmd)\n",
    "        os.system(build_cmd)\n",
    "    \n",
    "    label = \"hidden_{}_search_gt\".format(hidden_dim_count)\n",
    "    hidden_search_gt_results[label] = search_recall(\n",
    "        graph_path, \"gt_test_scores.bin\",\n",
    "        iterations=3, n_threads=N_SEARCH_THREADS,\n",
    "        runs_per_iteration=16\n",
    "    )\n",
    "    (lower_evals, upper_evals), _ = get_evals_for_recall(hidden_search_gt_results[label])\n",
    "    hidden_search_gt_evals.append((lower_evals, upper_evals))\n",
    "    \n",
    "    label = \"hidden_{}_search_rerank\".format(hidden_dim_count)\n",
    "    search_result_file = \"search_result_hidden_{}\".format(hidden_dim_count)\n",
    "    run_search(\n",
    "        graph_path,\n",
    "        \"hidden_{}_test_scores.bin\".format(hidden_dim_count),\n",
    "        topK=GT_TOP_LEN,\n",
    "        ef_ticks=[GT_TOP_LEN],\n",
    "        result_file=search_result_file,\n",
    "        n_threads=1,\n",
    "        verbose=False\n",
    "    )\n",
    "    approximate_top = read_txt(\n",
    "        \"data/synthetic/{}.txt\".format(search_result_file),\n",
    "        (QUERY_COUNT, GT_TOP_LEN)\n",
    "    )\n",
    "    gt = np.fromfile(\n",
    "        \"data/synthetic/data/model_scores/groundtruth_test.bin\",\n",
    "        dtype=\"int32\"\n",
    "    ).reshape((QUERY_COUNT, GT_TOP_LEN))\n",
    "    gt = gt[:,:RECALL_TOP_LEN]\n",
    "    result = calc_eval_recall_curve(approximate_top, gt)\n",
    "    hidden_search_rerank_results[label] = result\n",
    "    (lower_evals, upper_evals), _ = get_evals_for_recall(result)\n",
    "    hidden_search_rerank_evals.append((lower_evals, upper_evals))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build graph on relevance vectors\n",
    "relevance_graph_path = \"data/synthetic/rel_{}_graph.out\".format(QUERY_COUNT)\n",
    "if not os.path.isfile(relevance_graph_path) or REGENERATE_DATA:\n",
    "    build_cmd = build_graph_cmd_template.format(\n",
    "        featuresSize=QUERY_COUNT,\n",
    "        graphPath=relevance_graph_path,\n",
    "        relevanceVector=QUERY_COUNT,\n",
    "        features=\"model_scores/gt_train_scores\",\n",
    "        metric=\"l2\"\n",
    "    )\n",
    "    print(build_cmd)\n",
    "    os.system(build_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"relevance_search_gt\"\n",
    "relevance_search_gt_results[label] = search_recall(\n",
    "    relevance_graph_path, \"gt_test_scores.bin\",\n",
    "    iterations=3, n_threads=N_SEARCH_THREADS,\n",
    "    runs_per_iteration=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hidden_dim_count in HIDDEN_DIMENSIONS:\n",
    "    if hidden_dim_count == 0:\n",
    "        continue\n",
    "    label = \"relevance_search_hidden_{}\".format(hidden_dim_count)\n",
    "    search_result_file = \"search_result_rel_hidden_{}\".format(hidden_dim_count)\n",
    "    run_search(\n",
    "        relevance_graph_path,\n",
    "        \"hidden_{}_test_scores.bin\".format(hidden_dim_count),\n",
    "        topK=GT_TOP_LEN,\n",
    "        ef_ticks=[GT_TOP_LEN],\n",
    "        result_file=search_result_file,\n",
    "        n_threads=1,\n",
    "        verbose=False\n",
    "    )\n",
    "    approximate_top = read_txt(\n",
    "        \"data/synthetic/{}.txt\".format(search_result_file),\n",
    "        (QUERY_COUNT, GT_TOP_LEN)\n",
    "    )\n",
    "    gt = np.fromfile(\n",
    "        \"data/synthetic/data/model_scores/groundtruth_test.bin\",\n",
    "        dtype=\"int32\"\n",
    "    ).reshape((QUERY_COUNT, GT_TOP_LEN))\n",
    "    gt = gt[:,:RECALL_TOP_LEN]\n",
    "    result = calc_eval_recall_curve(approximate_top, gt)\n",
    "    relevance_search_gt_results[label] = result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build graph on relevance vectors based on catboost predictions\n",
    "\n",
    "model_path = \"data/synthetic/catboost_model.cbm\"\n",
    "if REGENERATE_DATA or not os.path.isfile(model_path):\n",
    "    X, y = sample_dataset(items, train_queries, samples_per_query=10000)\n",
    "    x_val, y_val = sample_dataset(items, train_queries, samples_per_query=1000)\n",
    "\n",
    "    model = CatBoostRegressor(iterations=1000, learning_rate=0.1)\n",
    "    model.fit(X, y, eval_set=(x_val, y_val), plot=True, verbose=False)\n",
    "    model.save_model(model_path)\n",
    "else:\n",
    "    model = CatBoostRegressor()\n",
    "    model.load_model(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REGENERATE_DATA:\n",
    "    make_predictions(items, train_queries, model, \"data/synthetic/data/model_scores/catboost_scores_train.bin\")\n",
    "    make_predictions(items, test_queries, model, \"data/synthetic/data/model_scores/catboost_scores_test.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make ground truth top by catboost scores\n",
    "\n",
    "catboost_gt_path = \"data/synthetic/data/model_scores/groundtruth_catboost.bin\"\n",
    "if not os.path.isfile(catboost_gt_path) or REGENERATE_DATA:\n",
    "    scores_path = \"data/synthetic/data/model_scores/catboost_scores_test.bin\"\n",
    "    scores = np.fromfile(scores_path, dtype=\"float32\").reshape(\n",
    "        (ITEM_COUNT, QUERY_COUNT))\n",
    "    gt = (-scores).argsort(axis=0)[:GT_TOP_LEN,:].T.astype(\"int32\")\n",
    "    gt.tofile(catboost_gt_path)\n",
    "    del gt\n",
    "    del scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_path = \"data/synthetic/catboost_rel_{}_graph.out\".format(QUERY_COUNT)\n",
    "if not os.path.isfile(graph_path) or REGENERATE_DATA:\n",
    "    build_cmd = build_graph_cmd_template.format(\n",
    "        featuresSize=QUERY_COUNT,\n",
    "        graphPath=graph_path,\n",
    "        relevanceVector=QUERY_COUNT,\n",
    "        features=\"model_scores/catboost_scores_train\",\n",
    "        metric=\"l2\"\n",
    "    )\n",
    "    print(build_cmd)\n",
    "    os.system(build_cmd)\n",
    "\n",
    "label = \"catboost_search_catboost\"\n",
    "relevance_search_gt_results[label] = search_recall(\n",
    "    graph_path, \"catboost_scores_test.bin\",\n",
    "    iterations=3, n_threads=N_SEARCH_THREADS,\n",
    "    runs_per_iteration=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"catboost_rank_catboost\"\n",
    "relevance_search_gt_results[label] = search_recall(\n",
    "    graph_path, \"catboost_scores_test.bin\",\n",
    "    iterations=3, n_threads=N_SEARCH_THREADS,\n",
    "    runs_per_iteration=16, gt_file=\"groundtruth_catboost.bin\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(*results, x_lim=None, y_lim=None, x_log_scale=False, keys=None):\n",
    "    plt.xlabel(\"evals\")\n",
    "    plt.ylabel(\"recall@5\")\n",
    "    if x_lim is not None:\n",
    "        plt.xlim(x_lim)\n",
    "    if y_lim is not None:\n",
    "        plt.ylim(y_lim)\n",
    "    if x_log_scale:\n",
    "        plt.xscale('log')\n",
    "    for result in results:\n",
    "        for label, stats in result.items():\n",
    "            if keys is None or label in keys:\n",
    "                plt.plot(stats[\"evals\"], stats[\"recall\"], label=label)\n",
    "                scatter_size = 10 if len(stats[\"evals\"]) < 100 else 0.1\n",
    "                plt.scatter(stats[\"evals\"], stats[\"recall\"], s=scatter_size)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plot_results(\n",
    "    hidden_search_rerank_results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plot_results(\n",
    "    hidden_search_gt_results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plot_results(\n",
    "    hidden_search_gt_results,\n",
    "    hidden_search_rerank_results,\n",
    "    relevance_search_gt_results,\n",
    "    keys=[\n",
    "        \"relevance_search_gt\",\n",
    "        \"hidden_1_search_rerank\", \"hidden_1_search_gt\",\n",
    "        \"hidden_2_search_rerank\", \"hidden_2_search_gt\",\n",
    "#         \"hidden_3_search_rerank\", \"hidden_3_search_gt\",\n",
    "#         \"hidden_0_search_gt\",\n",
    "#         \"catboost_search_catboost\", \"catboost_rank_catboost\",\n",
    "#         \"relevance_search_hidden_1\", \"relevance_search_hidden_2\"\n",
    "    ],\n",
    "    x_lim=(0, 10000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plot_results(\n",
    "    hidden_search_gt_results,\n",
    "    hidden_search_rerank_results,\n",
    "    relevance_search_gt_results,\n",
    "    keys=[\n",
    "        \"relevance_search_gt\",\n",
    "#         \"hidden_0_search_gt\",\n",
    "        \"catboost_search_catboost\", \"catboost_rank_catboost\",\n",
    "    ],\n",
    "    x_lim=(0, 1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plot_results(\n",
    "    hidden_search_gt_results,\n",
    "    hidden_search_rerank_results,\n",
    "    relevance_search_gt_results,\n",
    "    keys=[\n",
    "        \"relevance_search_gt\", \"hidden_0_search_gt\", \"hidden_0_search_rerank\"\n",
    "    ],\n",
    "    x_lim=(0, 1000),\n",
    "    y_lim=(0.8, 1.01)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from time import time\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "QUANTILES = [0.9, 0.99, 0.999, 1]\n",
    "\n",
    "def estimate_disorder(items, scores, gt_top=100, queries_to_sample=100, verbose=False):\n",
    "    item_count, query_count = scores.shape\n",
    "    chosen_queries = np.random.choice(query_count, queries_to_sample, replace=False)\n",
    "    gt = np.argsort(-scores[:,chosen_queries], axis=0)[:gt_top].T\n",
    "    \n",
    "    multipliers = []\n",
    "    start = time()\n",
    "    for i, q in enumerate(chosen_queries):\n",
    "        close_items = items[gt[i]]\n",
    "        ii_dists = pairwise_distances(close_items, items)\n",
    "        ii_ranks = ii_dists.argsort().argsort()\n",
    "        for rank_1 in range(1, gt_top):\n",
    "            for rank_2 in range(0, rank_1):\n",
    "                ii_rank = ii_ranks[rank_1][gt[i, rank_2]]\n",
    "                multipliers.append(ii_rank / (2 + rank_1 + rank_2))\n",
    "        if verbose:\n",
    "            print(\"{} queries processed, avg time: {}\".format(\n",
    "                i + 1, (time() - start) / (i + 1)\n",
    "            ))\n",
    "    return np.quantile(multipliers, QUANTILES)\n",
    "\n",
    "def make_table(disoreder_stats : OrderedDict):\n",
    "    columns = [str(q) for q in QUANTILES]\n",
    "    labels = []\n",
    "    data = []\n",
    "    for key, stats in disoreder_stats.items():\n",
    "        assert len(stats) == len(QUANTILES)\n",
    "        data.append(stats)\n",
    "        labels.append(key)\n",
    "    result = pd.DataFrame(data=data, columns=columns, index=labels)\n",
    "    return result.style.format(\"{:.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_58f26a9a_a6bc_11eb_adad_c11feaff4271\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >0.9</th>        <th class=\"col_heading level0 col1\" >0.99</th>        <th class=\"col_heading level0 col2\" >0.999</th>        <th class=\"col_heading level0 col3\" >1</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_58f26a9a_a6bc_11eb_adad_c11feaff4271level0_row0\" class=\"row_heading level0 row0\" >relevance_proximity</th>\n",
       "                        <td id=\"T_58f26a9a_a6bc_11eb_adad_c11feaff4271row0_col0\" class=\"data row0 col0\" >34.9</td>\n",
       "                        <td id=\"T_58f26a9a_a6bc_11eb_adad_c11feaff4271row0_col1\" class=\"data row0 col1\" >75.3</td>\n",
       "                        <td id=\"T_58f26a9a_a6bc_11eb_adad_c11feaff4271row0_col2\" class=\"data row0 col2\" >109.7</td>\n",
       "                        <td id=\"T_58f26a9a_a6bc_11eb_adad_c11feaff4271row0_col3\" class=\"data row0 col3\" >240.2</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_58f26a9a_a6bc_11eb_adad_c11feaff4271level0_row1\" class=\"row_heading level0 row1\" >hidden_0</th>\n",
       "                        <td id=\"T_58f26a9a_a6bc_11eb_adad_c11feaff4271row1_col0\" class=\"data row1 col0\" >33.8</td>\n",
       "                        <td id=\"T_58f26a9a_a6bc_11eb_adad_c11feaff4271row1_col1\" class=\"data row1 col1\" >71.7</td>\n",
       "                        <td id=\"T_58f26a9a_a6bc_11eb_adad_c11feaff4271row1_col2\" class=\"data row1 col2\" >104.0</td>\n",
       "                        <td id=\"T_58f26a9a_a6bc_11eb_adad_c11feaff4271row1_col3\" class=\"data row1 col3\" >297.8</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_58f26a9a_a6bc_11eb_adad_c11feaff4271level0_row2\" class=\"row_heading level0 row2\" >hidden_1</th>\n",
       "                        <td id=\"T_58f26a9a_a6bc_11eb_adad_c11feaff4271row2_col0\" class=\"data row2 col0\" >61.7</td>\n",
       "                        <td id=\"T_58f26a9a_a6bc_11eb_adad_c11feaff4271row2_col1\" class=\"data row2 col1\" >142.4</td>\n",
       "                        <td id=\"T_58f26a9a_a6bc_11eb_adad_c11feaff4271row2_col2\" class=\"data row2 col2\" >224.2</td>\n",
       "                        <td id=\"T_58f26a9a_a6bc_11eb_adad_c11feaff4271row2_col3\" class=\"data row2 col3\" >684.8</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_58f26a9a_a6bc_11eb_adad_c11feaff4271level0_row3\" class=\"row_heading level0 row3\" >hidden_2</th>\n",
       "                        <td id=\"T_58f26a9a_a6bc_11eb_adad_c11feaff4271row3_col0\" class=\"data row3 col0\" >118.4</td>\n",
       "                        <td id=\"T_58f26a9a_a6bc_11eb_adad_c11feaff4271row3_col1\" class=\"data row3 col1\" >305.4</td>\n",
       "                        <td id=\"T_58f26a9a_a6bc_11eb_adad_c11feaff4271row3_col2\" class=\"data row3 col2\" >548.3</td>\n",
       "                        <td id=\"T_58f26a9a_a6bc_11eb_adad_c11feaff4271row3_col3\" class=\"data row3 col3\" >2025.4</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_58f26a9a_a6bc_11eb_adad_c11feaff4271level0_row4\" class=\"row_heading level0 row4\" >hidden_3</th>\n",
       "                        <td id=\"T_58f26a9a_a6bc_11eb_adad_c11feaff4271row4_col0\" class=\"data row4 col0\" >209.0</td>\n",
       "                        <td id=\"T_58f26a9a_a6bc_11eb_adad_c11feaff4271row4_col1\" class=\"data row4 col1\" >565.2</td>\n",
       "                        <td id=\"T_58f26a9a_a6bc_11eb_adad_c11feaff4271row4_col2\" class=\"data row4 col2\" >1074.4</td>\n",
       "                        <td id=\"T_58f26a9a_a6bc_11eb_adad_c11feaff4271row4_col3\" class=\"data row4 col3\" >3524.5</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f9ac8f26070>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disorder_stats_path = \"data/{}/data/disorder_statistics.json\".format(DATASET)\n",
    "\n",
    "\n",
    "if not os.path.isfile(disorder_stats_path) or REGENERATE_DATA:\n",
    "    train_queries = np.fromfile(\n",
    "        \"data/{}/data/train_queries.bin\".format(DATASET),\n",
    "        dtype=\"float32\"\n",
    "    ).reshape((QUERY_COUNT, DIMENSION))\n",
    "    test_queries = np.fromfile(\n",
    "        \"data/{}/data/test_queries.bin\".format(DATASET),\n",
    "        dtype=\"float32\"\n",
    "    ).reshape((QUERY_COUNT, DIMENSION))\n",
    "    items = np.fromfile(\n",
    "        \"data/{}/data/items.bin\".format(DATASET),\n",
    "        dtype=\"float32\"\n",
    "    ).reshape((ITEM_COUNT, DIMENSION))\n",
    "\n",
    "    disorder_stats = OrderedDict()\n",
    "    item_features = np.fromfile(\n",
    "        \"data/{}/data/model_scores/gt_train_scores.bin\".format(DATASET),\n",
    "        dtype=\"float32\"\n",
    "    ).reshape(ITEM_COUNT, QUERY_COUNT)\n",
    "    relevance_scores = np.fromfile(\n",
    "        \"data/{}/data/model_scores/gt_test_scores.bin\".format(DATASET),\n",
    "        dtype=\"float32\"\n",
    "    ).reshape(ITEM_COUNT, QUERY_COUNT)\n",
    "\n",
    "    disorder_stats[\"relevance_proximity\"] = estimate_disorder(\n",
    "        item_features, relevance_scores,\n",
    "        queries_to_sample=100\n",
    "    )\n",
    "    del item_features\n",
    "\n",
    "    for hidden_dim_count in HIDDEN_DIMENSIONS:\n",
    "        open_dims = DIMENSION - hidden_dim_count\n",
    "        disorder_stats[\"hidden_{}\".format(hidden_dim_count)] = estimate_disorder(\n",
    "            items[:,:open_dims], relevance_scores\n",
    "        )\n",
    "    del relevance_scores\n",
    "\n",
    "    for k in disorder_stats:\n",
    "        disorder_stats[k] = [float(num) for num in disorder_stats[k]]\n",
    "    with open(disorder_stats_path, \"w\") as fout:\n",
    "        json.dump(disorder_stats, fout, indent=4)\n",
    "else:\n",
    "    with open(disorder_stats_path) as fin:\n",
    "        disorder_stats = json.load(fin)\n",
    "\n",
    "make_table(disorder_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_f90a6ae8_a6ba_11eb_adad_c11feaff4271\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >0.9</th>        <th class=\"col_heading level0 col1\" >0.99</th>        <th class=\"col_heading level0 col2\" >0.999</th>        <th class=\"col_heading level0 col3\" >1</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_f90a6ae8_a6ba_11eb_adad_c11feaff4271level0_row0\" class=\"row_heading level0 row0\" >relevance_proximity</th>\n",
       "                        <td id=\"T_f90a6ae8_a6ba_11eb_adad_c11feaff4271row0_col0\" class=\"data row0 col0\" >34.9</td>\n",
       "                        <td id=\"T_f90a6ae8_a6ba_11eb_adad_c11feaff4271row0_col1\" class=\"data row0 col1\" >75.3</td>\n",
       "                        <td id=\"T_f90a6ae8_a6ba_11eb_adad_c11feaff4271row0_col2\" class=\"data row0 col2\" >109.7</td>\n",
       "                        <td id=\"T_f90a6ae8_a6ba_11eb_adad_c11feaff4271row0_col3\" class=\"data row0 col3\" >240.2</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f90a6ae8_a6ba_11eb_adad_c11feaff4271level0_row1\" class=\"row_heading level0 row1\" >hidden_0</th>\n",
       "                        <td id=\"T_f90a6ae8_a6ba_11eb_adad_c11feaff4271row1_col0\" class=\"data row1 col0\" >33.8</td>\n",
       "                        <td id=\"T_f90a6ae8_a6ba_11eb_adad_c11feaff4271row1_col1\" class=\"data row1 col1\" >71.7</td>\n",
       "                        <td id=\"T_f90a6ae8_a6ba_11eb_adad_c11feaff4271row1_col2\" class=\"data row1 col2\" >104.0</td>\n",
       "                        <td id=\"T_f90a6ae8_a6ba_11eb_adad_c11feaff4271row1_col3\" class=\"data row1 col3\" >297.8</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f90a6ae8_a6ba_11eb_adad_c11feaff4271level0_row2\" class=\"row_heading level0 row2\" >hidden_1</th>\n",
       "                        <td id=\"T_f90a6ae8_a6ba_11eb_adad_c11feaff4271row2_col0\" class=\"data row2 col0\" >61.7</td>\n",
       "                        <td id=\"T_f90a6ae8_a6ba_11eb_adad_c11feaff4271row2_col1\" class=\"data row2 col1\" >142.4</td>\n",
       "                        <td id=\"T_f90a6ae8_a6ba_11eb_adad_c11feaff4271row2_col2\" class=\"data row2 col2\" >224.2</td>\n",
       "                        <td id=\"T_f90a6ae8_a6ba_11eb_adad_c11feaff4271row2_col3\" class=\"data row2 col3\" >684.8</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f90a6ae8_a6ba_11eb_adad_c11feaff4271level0_row3\" class=\"row_heading level0 row3\" >hidden_2</th>\n",
       "                        <td id=\"T_f90a6ae8_a6ba_11eb_adad_c11feaff4271row3_col0\" class=\"data row3 col0\" >118.4</td>\n",
       "                        <td id=\"T_f90a6ae8_a6ba_11eb_adad_c11feaff4271row3_col1\" class=\"data row3 col1\" >305.4</td>\n",
       "                        <td id=\"T_f90a6ae8_a6ba_11eb_adad_c11feaff4271row3_col2\" class=\"data row3 col2\" >548.3</td>\n",
       "                        <td id=\"T_f90a6ae8_a6ba_11eb_adad_c11feaff4271row3_col3\" class=\"data row3 col3\" >2025.4</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f90a6ae8_a6ba_11eb_adad_c11feaff4271level0_row4\" class=\"row_heading level0 row4\" >hidden_3</th>\n",
       "                        <td id=\"T_f90a6ae8_a6ba_11eb_adad_c11feaff4271row4_col0\" class=\"data row4 col0\" >209.0</td>\n",
       "                        <td id=\"T_f90a6ae8_a6ba_11eb_adad_c11feaff4271row4_col1\" class=\"data row4 col1\" >565.2</td>\n",
       "                        <td id=\"T_f90a6ae8_a6ba_11eb_adad_c11feaff4271row4_col2\" class=\"data row4 col2\" >1074.4</td>\n",
       "                        <td id=\"T_f90a6ae8_a6ba_11eb_adad_c11feaff4271row4_col3\" class=\"data row4 col3\" >3524.5</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f9ac98e9eb0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_table(disorder_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "disorder_stats_path = \"data/{}/data/disorder_statistics.json\".format(DATASET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
