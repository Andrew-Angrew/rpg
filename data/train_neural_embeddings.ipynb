{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fvecs(file_name):\n",
    "    a = np.fromfile(file_name, dtype=\"int32\")\n",
    "    dim = a[0]\n",
    "    return a.view(\"float32\").reshape((-1, dim + 1))[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"collections\"\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 128\n",
    "\n",
    "ITEM_BATCH_SIZE = 10000\n",
    "QUERY_BATCH_SIZE = 100\n",
    "CLOSE_ITEM_COUNT = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features = read_fvecs(\"{}/data/item_features.fvecs\".format(DATASET))\n",
    "train_query_features = read_fvecs(\"{}/data/user_features_train.fvecs\".format(DATASET))\n",
    "test_query_features = read_fvecs(\"{}/data/user_features_test.fvecs\".format(DATASET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def measure_scattering(data):\n",
    "#     scattering = data.max(axis=0) - data.min(axis=0)\n",
    "#     scattering /= data.std(axis=0)\n",
    "#     return scattering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEM_COUNT, ITEM_FEATURES_COUNT = item_features.shape\n",
    "QUERY_COUNT, QUERY_FEATURES_COUNT = train_query_features.shape\n",
    "VALIDATION_QUERY_COUNT = 100\n",
    "TRAIN_QUERY_COUNT = QUERY_COUNT - VALIDATION_QUERY_COUNT\n",
    "\n",
    "train_query_indexes, val_query_indexes = train_test_split(\n",
    "    np.arange(QUERY_COUNT), test_size=VALIDATION_QUERY_COUNT,\n",
    "    random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.fromfile(\n",
    "    \"{}/data/model_scores/scores_train.bin\".format(DATASET),\n",
    "    dtype=\"float32\"\n",
    ").reshape((ITEM_COUNT, QUERY_COUNT))\n",
    "target = torch.FloatTensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_items = np.fromfile(\n",
    "    \"{}/data/model_scores/groundtruth_train.bin\".format(DATASET),\n",
    "    dtype=\"int32\"\n",
    ").reshape(QUERY_COUNT, CLOSE_ITEM_COUNT)\n",
    "close_probs = 1. / np.arange(1, CLOSE_ITEM_COUNT + 1)\n",
    "close_probs /= close_probs.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_scaler = QuantileTransformer()\n",
    "item_features = item_scaler.fit_transform(item_features)\n",
    "\n",
    "query_scaler = QuantileTransformer()\n",
    "train_query_features = query_scaler.fit_transform(train_query_features)\n",
    "test_query_features = query_scaler.transform(test_query_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features = torch.FloatTensor(item_features)\n",
    "train_query_features = torch.FloatTensor(train_query_features)\n",
    "test_query_features = torch.FloatTensor(test_query_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_net = nn.Sequential(\n",
    "    nn.Linear(in_features=ITEM_FEATURES_COUNT, out_features=HIDDEN_DIM, bias=True),\n",
    "    nn.BatchNorm1d(HIDDEN_DIM),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(in_features=HIDDEN_DIM, out_features=HIDDEN_DIM, bias=True),\n",
    "    nn.BatchNorm1d(HIDDEN_DIM),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(in_features=HIDDEN_DIM, out_features=EMBEDDING_DIM, bias=True),\n",
    ")\n",
    "\n",
    "query_net = nn.Sequential(\n",
    "    nn.Linear(in_features=QUERY_FEATURES_COUNT, out_features=HIDDEN_DIM, bias=True),\n",
    "    nn.BatchNorm1d(HIDDEN_DIM),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(in_features=HIDDEN_DIM, out_features=HIDDEN_DIM, bias=True),\n",
    "    nn.BatchNorm1d(HIDDEN_DIM),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(in_features=HIDDEN_DIM, out_features=EMBEDDING_DIM, bias=True),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(\n",
    "#     params=list(item_net.parameters()) + list(query_net.parameters()),\n",
    "#     lr=0.01, weight_decay=0.1\n",
    "# )\n",
    "optimizer = torch.optim.SGD(\n",
    "    params=list(item_net.parameters()) + list(query_net.parameters()),\n",
    "    lr=0.001, weight_decay=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.022129510098033482           Validation loss: 0.014315928816795349          \n",
      "Train loss: 0.01157816696829266            Validation loss: 0.010313705563545227          \n",
      "Train loss: 0.009377706574069129           Validation loss: 0.009026462733745575          \n",
      "Train loss: 0.008474990427494049           Validation loss: 0.008391676664352418          \n",
      "Train loss: 0.007956832223468357           Validation loss: 0.00797242033481598           \n",
      "Train loss: 0.007617523729801178           Validation loss: 0.007679654002189637          \n",
      "Train loss: 0.007360780808660719           Validation loss: 0.007427020072937012          \n",
      "Train loss: 0.007151115225421058           Validation loss: 0.007273702263832092          \n",
      "Train loss: 0.006992318438159095           Validation loss: 0.007137261748313904          \n",
      "Train loss: 0.0068642560972107785          Validation loss: 0.0070020023584365845         \n",
      "Train loss: 0.006756950325436062           Validation loss: 0.006921856641769409          \n",
      "Train loss: 0.0066580798361036515          Validation loss: 0.006826439797878265          \n",
      "Train loss: 0.0065754616922802395          Validation loss: 0.006723830044269562          \n",
      "Train loss: 0.0065110401643647085          Validation loss: 0.0067098535299301145         \n",
      "Train loss: 0.006450023611386617           Validation loss: 0.006585752964019776          \n",
      "Train loss: 0.006393743620978462           Validation loss: 0.0065665792226791385         \n",
      "Train loss: 0.006343716508812374           Validation loss: 0.006490558743476867          \n",
      "Train loss: 0.0062969215975867375          Validation loss: 0.006476687669754028          \n",
      "Train loss: 0.006258455687099033           Validation loss: 0.006426543593406677          \n",
      "Train loss: 0.0062203991280661685          Validation loss: 0.006423974990844726          \n",
      "Train loss: 0.006188205626275804           Validation loss: 0.0063584036231040955         \n",
      "Train loss: 0.00616281846496794            Validation loss: 0.0063541786670684814         \n",
      "Train loss: 0.006128769596417745           Validation loss: 0.006326078534126282          \n",
      "Train loss: 0.0061112688382466635          Validation loss: 0.006312169253826141          \n",
      "Train loss: 0.006084851251708137           Validation loss: 0.006243005633354187          \n",
      "Train loss: 0.006065859701898363           Validation loss: 0.006254202425479889          \n",
      "Train loss: 0.006046343366305033           Validation loss: 0.006254873871803283          \n",
      "Train loss: 0.006028371883763207           Validation loss: 0.006211229681968689          \n",
      "Train loss: 0.006011913008160061           Validation loss: 0.006201585114002228          \n",
      "Train loss: 0.005987056189113193           Validation loss: 0.0061643973588943485         \n",
      "Train loss: 0.0059788283771938745          Validation loss: 0.006178054451942444          \n",
      "Train loss: 0.005966581086317698           Validation loss: 0.0061300876140594485         \n",
      "Train loss: 0.005957104735904269           Validation loss: 0.0061401987075805665         \n",
      "Train loss: 0.0059433465798695885          Validation loss: 0.006114902436733246          \n",
      "Train loss: 0.005927274511920082           Validation loss: 0.006120069146156311          \n",
      "Train loss: 0.005920020944542355           Validation loss: 0.0061011951565742495         \n",
      "Train loss: 0.005908801244364844           Validation loss: 0.006094285368919373          \n",
      "Train loss: 0.005897894601027171           Validation loss: 0.006094857394695282          \n",
      "Train loss: 0.005895912269751231           Validation loss: 0.006086545526981354          \n",
      "Train loss: 0.005888543870713975           Validation loss: 0.006066266119480133          \n",
      "Train loss: 0.005873161017894745           Validation loss: 0.006075246691703796          \n",
      "Train loss: 0.00586294025182724            Validation loss: 0.006053072810173035          \n",
      "Train loss: 0.005855197528998057           Validation loss: 0.006035540580749512          \n",
      "Train loss: 0.005856423497200012           Validation loss: 0.0060389344692230226         \n",
      "Train loss: 0.005850322948561774           Validation loss: 0.00604773223400116           \n",
      "Train loss: 0.005841451876693301           Validation loss: 0.006019276022911072          \n",
      "Train loss: 0.00583867801560296            Validation loss: 0.006014438331127167          \n",
      "Train loss: 0.005829856309625838           Validation loss: 0.00601954573392868           \n",
      "Train loss: 0.005830603222052256           Validation loss: 0.006010477602481842          \n",
      "Train loss: 0.005824257082409329           Validation loss: 0.006013207733631134          \n",
      "Train loss: 0.005816898922125498           Validation loss: 0.0060145086646080015         \n",
      "Train loss: 0.0058186393181482955          Validation loss: 0.005982832491397858          \n",
      "Train loss: 0.0058115417493714225          Validation loss: 0.006005559027194977          \n",
      "Train loss: 0.00581152456998825            Validation loss: 0.005996121883392334          \n",
      "Train loss: 0.005805761218070984           Validation loss: 0.005977740049362182          \n",
      "Train loss: 0.005800928559568193           Validation loss: 0.005984664976596833          \n",
      "Train loss: 0.005801896618472205           Validation loss: 0.005996375799179077          \n",
      "Train loss: 0.00579960032304128            Validation loss: 0.005957635462284088          \n",
      "Train loss: 0.005789407809575399           Validation loss: 0.005961443603038788          \n",
      "Train loss: 0.005794557902548048           Validation loss: 0.005977129936218262          \n",
      "Train loss: 0.005789446506235334           Validation loss: 0.00595981627702713           \n",
      "Train loss: 0.005780314359400007           Validation loss: 0.005938316881656647          \n",
      "Train loss: 0.0057812785771158             Validation loss: 0.005965632200241089          \n",
      "Train loss: 0.005778869569301605           Validation loss: 0.0059836809635162354         \n",
      "Train loss: 0.005777885297934215           Validation loss: 0.005952526926994324          \n",
      "Train loss: 0.005783785932593876           Validation loss: 0.005953099548816681          \n",
      "Train loss: 0.005774464819166395           Validation loss: 0.005945719838142395          \n",
      "Train loss: 0.005774641513824463           Validation loss: 0.0059403377175331114         \n",
      "Train loss: 0.005774395134713914           Validation loss: 0.005945308804512024          \n",
      "Train loss: 0.0057680003179444205          Validation loss: 0.005962152004241944          \n",
      "Train loss: 0.00576585904094908            Validation loss: 0.005940976023674011          \n",
      "Train loss: 0.0057661999066670735          Validation loss: 0.005932345509529114          \n",
      "Train loss: 0.005767874187893337           Validation loss: 0.00591660338640213           \n",
      "Train loss: 0.005772408650981055           Validation loss: 0.005937310814857483          \n",
      "Train loss: 0.005765369951725006           Validation loss: 0.005954287350177765          \n",
      "Train loss: 0.005766765693823496           Validation loss: 0.005934203147888183          \n",
      "Train loss: 0.005760922915405697           Validation loss: 0.00594174462556839           \n",
      "Train loss: 0.0057627594272295635          Validation loss: 0.005931388735771179          \n",
      "Train loss: 0.005762819773621029           Validation loss: 0.005941673278808594          \n",
      "Train loss: 0.005764912048975627           Validation loss: 0.005923346102237701          \n",
      "Train loss: 0.0057598522570398115          Validation loss: 0.005924840092658997          \n",
      "Train loss: 0.005765092492103577           Validation loss: 0.005931110262870788          \n",
      "Train loss: 0.005764687604374355           Validation loss: 0.005925367593765259          \n",
      "Train loss: 0.005759577943219079           Validation loss: 0.005935010194778443          \n"
     ]
    }
   ],
   "source": [
    "def calc_batch_loss_stupid(query_indexes, train=True):\n",
    "    batch_queries = train_query_features[query_indexes]\n",
    "    \n",
    "    batch_item_indexes = np.random.choice(ITEM_COUNT, ITEM_BATCH_SIZE, replace=False)\n",
    "    batch_items = item_features[batch_item_indexes]\n",
    "    batch_target = target[batch_item_indexes][:,query_indexes]\n",
    "\n",
    "    if train:\n",
    "        item_net.train()\n",
    "        query_net.train()\n",
    "    else:\n",
    "        item_net.eval()\n",
    "        query_net.eval()\n",
    "\n",
    "    batch_item_embeds = item_net(batch_items)\n",
    "    batch_query_embeds = query_net(batch_queries)\n",
    "    relevance_prediction = torch.matmul(batch_item_embeds, batch_query_embeds.T)\n",
    "    loss = ((batch_target - relevance_prediction) ** 2).mean()\n",
    "    return loss\n",
    "\n",
    "def calc_batch_loss(query_indexes, train=True):\n",
    "    batch_queries = train_query_features[query_indexes]\n",
    "    \n",
    "    positives_ranks = np.random.choice(\n",
    "        CLOSE_ITEM_COUNT, len(query_indexes), p=close_probs)\n",
    "    positives_indexes = close_items[query_indexes, positives_ranks]\n",
    "    batch_posives = item_features[positives_indexes]\n",
    "    positive_target = target[positives_indexes, query_indexes]\n",
    "\n",
    "    negative_indexes = np.random.choice(ITEM_COUNT, ITEM_BATCH_SIZE, replace=False)\n",
    "    batch_negatives = item_features[negative_indexes]\n",
    "    negative_target = target[negative_indexes][:,query_indexes]\n",
    "\n",
    "    if train:\n",
    "        item_net.train()\n",
    "        query_net.train()\n",
    "    else:\n",
    "        item_net.eval()\n",
    "        query_net.eval()\n",
    "\n",
    "    batch_query_embeds = query_net(batch_queries)\n",
    "    \n",
    "    positive_item_embeds = item_net(batch_posives)\n",
    "    positive_relevance_prediction = (positive_item_embeds * batch_query_embeds).sum(axis=1)\n",
    "    positive_loss = ((positive_target - positive_relevance_prediction) ** 2).mean()\n",
    "    \n",
    "    negative_item_embeds = item_net(batch_negatives)\n",
    "    negative_relevance_prediction = torch.matmul(negative_item_embeds, batch_query_embeds.T)\n",
    "    negative_loss = ((negative_target - negative_relevance_prediction) ** 2).mean()\n",
    "    return positive_loss + negative_loss\n",
    "\n",
    "\n",
    "item_model_path = \"collections/item.net\"\n",
    "query_model_path = \"collections/query.net\"\n",
    "best_loss = None\n",
    "best_epoch = 0\n",
    "PASSES_PER_EPOCH = 10\n",
    "\n",
    "for epoch in range(100):\n",
    "    train_loss = 0\n",
    "    validation_loss = 0\n",
    "    \n",
    "    for i in range(PASSES_PER_EPOCH):\n",
    "        query_permutation = np.random.permutation(train_query_indexes)\n",
    "        for batch_start in range(0, TRAIN_QUERY_COUNT, QUERY_BATCH_SIZE):\n",
    "            batch_query_indexes = query_permutation[batch_start: batch_start + QUERY_BATCH_SIZE]\n",
    "            loss = calc_batch_loss_stupid(batch_query_indexes)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.data.item()\n",
    "    train_loss /= TRAIN_QUERY_COUNT * PASSES_PER_EPOCH\n",
    "    for i in range(PASSES_PER_EPOCH):\n",
    "        validation_loss += calc_batch_loss_stupid(\n",
    "            val_query_indexes, train=False).data.item()\n",
    "    validation_loss /= VALIDATION_QUERY_COUNT * PASSES_PER_EPOCH\n",
    "    \n",
    "    if best_loss is None or validation_loss < best_loss:\n",
    "        best_loss = validation_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(item_net, item_model_path)\n",
    "        torch.save(query_net, query_model_path)\n",
    "        \n",
    "    print(\"Train loss: {:<30} Validation loss: {:<30}\".format(train_loss, validation_loss))\n",
    "    if epoch > best_epoch + 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 0.00591660338640213)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_epoch, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_net = torch.load(item_model_path)\n",
    "query_net = torch.load(query_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_item_net = torch.load(item_model_path)\n",
    "best_query_net = torch.load(query_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_item_net.eval()\n",
    "best_query_net.eval()\n",
    "\n",
    "item_embeddings = best_item_net(item_features).detach().numpy()\n",
    "item_embeddings.astype(\"float32\").tofile(\"{}/data/item_embeddings.bin\".format(DATASET))\n",
    "\n",
    "train_query_embeddings = best_query_net(train_query_features).detach().numpy()\n",
    "train_query_embeddings.astype(\"float32\").tofile(\"{}/data/query_embeddings_train.bin\".format(DATASET))\n",
    "\n",
    "test_query_embeddings = best_query_net(test_query_features).detach().numpy()\n",
    "test_query_embeddings.astype(\"float32\").tofile(\"{}/data/query_embeddings_test.bin\".format(DATASET))\n",
    "\n",
    "embedding_train_scores = item_embeddings.dot(train_query_embeddings.T)\n",
    "embedding_train_scores.astype(\"float32\").tofile(\n",
    "    \"{}/data/model_scores/embedding_scores_train.bin\".format(DATASET)\n",
    ")\n",
    "del embedding_train_scores\n",
    "\n",
    "embedding_test_scores = item_embeddings.dot(test_query_embeddings.T)\n",
    "embedding_test_scores.astype(\"float32\").tofile(\n",
    "    \"{}/data/model_scores/embedding_scores_test.bin\".format(DATASET)\n",
    ")\n",
    "del embedding_test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
