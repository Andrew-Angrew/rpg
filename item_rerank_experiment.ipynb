{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np\n",
    "import h5py\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from optimal_scheduling import calc_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEM_COUNT = 10000\n",
    "LEARN_COUNT = 1000\n",
    "VALIDATION_COUNT = 1000\n",
    "QUERY_COUNT = 1000\n",
    "GT_TOP = 5\n",
    "\n",
    "efSearchTicks = list(np.exp(np.linspace(np.log(10), np.log(1000), 31)).astype(\"int\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(a):\n",
    "    vec_lengths = np.sqrt(np.power(a, 2).sum(axis=1, keepdims=True))\n",
    "    return a / vec_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pairwise_relevance(i, q):\n",
    "    return 1 - pairwise_distances(i, q, metric=\"cosine\", n_jobs=-1).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get glove data\n"
     ]
    }
   ],
   "source": [
    "print(\"Get glove data\")\n",
    "with h5py.File(\"data/glove/glove-25-angular.hdf5\", \"r\") as f:\n",
    "    glove_base = normalize(f[\"train\"][:])\n",
    "    glove_query = normalize(f['test'][:])\n",
    "\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(glove_base)\n",
    "np.random.shuffle(glove_query)\n",
    "\n",
    "items = glove_base[:ITEM_COUNT]\n",
    "\n",
    "train_queries = glove_query[:LEARN_COUNT]\n",
    "validation_queries = glove_query[LEARN_COUNT: LEARN_COUNT + VALIDATION_COUNT]\n",
    "test_queries = glove_query[-QUERY_COUNT:]\n",
    "\n",
    "items.tofile(\"data/reranking/data/items.bin\")\n",
    "test_relevances = calc_pairwise_relevance(items, test_queries)\n",
    "test_relevances.tofile(\n",
    "    \"data/reranking/data/test_relevances.bin\"\n",
    ")\n",
    "ground_truth = np.argsort(-test_relevances, axis=0)[:GT_TOP].T.astype(\"int32\")\n",
    "ground_truth.tofile(\n",
    "    \"data/reranking/data/groundtruth.bin\"\n",
    ")\n",
    "\n",
    "#     train_queries.tofile(\"data/{}/data/train_queries.bin\".format(DATASET))\n",
    "#     test_queries.tofile(\"data/{}/data/test_queries.bin\".format(DATASET))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_item_order(items):\n",
    "    distances = pairwise_distances(items)\n",
    "    item_order = np.argsort(distances, axis=1)\n",
    "    return item_order\n",
    "\n",
    "def order_to_ranks(item_order):\n",
    "    n = item_order.shape[0]\n",
    "    assert item_order.shape == (n, n)\n",
    "    item_ranks = np.empty((n, n), dtype=\"int32\")\n",
    "    ranks = np.arange(n)\n",
    "    for i in range(n):\n",
    "        item_ranks[i][item_order[i]] = ranks\n",
    "    return item_ranks\n",
    "\n",
    "def calc_pairwise_ranks(items):\n",
    "    item_order = calc_item_order(items)\n",
    "    return order_to_ranks(item_order)\n",
    "\n",
    "def calc_item_query_ranks(items, queries):\n",
    "    item_count = items.shape[0]\n",
    "    query_count = queries.shape[0]\n",
    "    relevances = calc_pairwise_relevance(items, queries)\n",
    "    item_order = np.argsort(-relevances, axis=0)\n",
    "    item_ranks = np.empty((item_count, query_count), dtype=\"int32\")\n",
    "    ranks = np.arange(1, item_count + 1, dtype=\"int32\")\n",
    "    for i in range(query_count):\n",
    "        item_ranks[:,i][item_order[:,i]] = ranks\n",
    "    return item_ranks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(\n",
    "    calc_pairwise_ranks(np.array([\n",
    "        [0, 1],\n",
    "        [0, 0],\n",
    "        [2, 0],\n",
    "    ])) == np.array([\n",
    "        [0, 1, 2],\n",
    "        [1, 0, 2],\n",
    "        [2, 1, 0]\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_min_sum_ranks(item_query_ranks):\n",
    "    return pairwise_distances(item_query_ranks, metric=lambda a, b: (a + b).min())\n",
    "\n",
    "def calc_disorder(item_item_ranks, item_query_ranks):\n",
    "    min_sum_ranks = calc_min_sum_ranks(item_query_ranks)\n",
    "    return np.max(item_item_ranks / min_sum_ranks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q_ranks = calc_item_query_ranks(items, train_queries)\n",
    "metric_item_item_ranks = calc_pairwise_ranks(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1725.6666666666667"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_disorder(metric_item_item_ranks, train_q_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_min_sum_ranks = calc_min_sum_ranks(train_q_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_train_item_order = train_min_sum_ranks.argsort(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_train_item_ranks = order_to_ranks(greedy_train_item_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.82608695652174"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(greedy_train_item_ranks / train_min_sum_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1725.6666666666667"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(metric_item_item_ranks / train_min_sum_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_q_ranks = calc_item_query_ranks(items, validation_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_min_sum_ranks = calc_min_sum_ranks(validation_q_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1370.5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(metric_item_item_ranks / validation_min_sum_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2354.6666666666665"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(greedy_train_item_ranks / validation_min_sum_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_items(item_order, min_sum_ranks, D):\n",
    "    n = item_order.shape[0]\n",
    "    assert item_order.shape == (n, n)\n",
    "    assert min_sum_ranks.shape == (n, n)\n",
    "    \n",
    "    result = np.empty((n, n), dtype=\"int32\")\n",
    "    for i, order in enumerate(item_order):\n",
    "        deadlines = [min(int(num), n - 1) for num in D * min_sum_ranks[i]]\n",
    "        deadlines = [deadlines[i] for i in order]\n",
    "        rearrangement = calc_schedule(deadlines)\n",
    "        if rearrangement is None:\n",
    "            return None\n",
    "        result[i] = order[rearrangement]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_item_order = calc_item_order(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rearr_order = reorder_items(metric_item_order, train_min_sum_ranks, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rearr_order is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rearr_ranks = order_to_ranks(rearr_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1370.75"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(rearr_ranks / validation_min_sum_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(rearr_ranks / train_min_sum_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_train_item_ranks.astype(\"float32\").tofile(\"data/reranking/rerank_greedy.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_item_item_ranks.astype(\"float32\").tofile(\"data/reranking/rerank_trivial.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_RANGE = [17, 30, 50, 100, 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 1395.25 17.0\n"
     ]
    }
   ],
   "source": [
    "for D in D_RANGE:\n",
    "    rearr_order = reorder_items(metric_item_order, train_min_sum_ranks, D)\n",
    "    assert rearr_order is not None\n",
    "    rearr_ranks = order_to_ranks(rearr_order)\n",
    "    print(D, np.max(rearr_ranks / validation_min_sum_ranks), np.max(rearr_ranks / train_min_sum_ranks))\n",
    "    rearr_ranks.astype(\"float32\").tofile(\n",
    "        \"data/reranking/rerank_{}.bin\".format(D)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_graph_cmd_template = (\n",
    "    \"./RPG --mode base \"\n",
    "    \"--baseSize 10000 \"\n",
    "    \"--trainQueries {featuresSize} \"\n",
    "    \"--base data/reranking/{features}.bin \"\n",
    "    \"--outputGraph {graphPath} \"\n",
    "    \"--relevanceVector {featuresSize} \"\n",
    "    \"--efConstruction 1000 --M 8 \"\n",
    "    \"--metric {metric}\"\n",
    ")\n",
    "\n",
    "def build_graph(dataset, graph_path, features, dim, metric, recalc=False):\n",
    "    if os.path.isfile(graph_path) and not recalc:\n",
    "        return\n",
    "    cmd = build_graph_cmd_template.format(\n",
    "        dataset=dataset,\n",
    "        featuresSize=dim,\n",
    "        features=features,\n",
    "        graphPath=graph_path,\n",
    "        metric=metric\n",
    "    )\n",
    "    print(cmd)\n",
    "    os.system(cmd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_cmd_template = (\n",
    "    \"./RPG --mode query --baseSize 10000 --querySize 1000\"\n",
    "    \" --query data/reranking/data/test_relevances.bin --inputGraph {inputGraph}\"\n",
    "    \" --efSearch {efSearch} --topK 5 --output data/reranking/search_result.txt\" +\n",
    "    \" --gtQueries 1000 --gtTop {} \".format(GT_TOP) +\n",
    "    \"--groundtruth data/reranking/data/groundtruth.bin\"\n",
    ")\n",
    "\n",
    "def bench_cmd(cmd):\n",
    "    res = {}\n",
    "    cmd_out = []\n",
    "    with os.popen(cmd) as out:\n",
    "        for line in out:\n",
    "            cmd_out.append(line)\n",
    "            for stat_name, prefix, suff_len in [\n",
    "                (\"relevance\", \"Average relevance: \", 1),\n",
    "                (\"recall\", \"Recall@5: \", 1),\n",
    "                (\"time\", \"Average query time: \", 3),\n",
    "                (\"evals\", \"Average number of model computations: \", 1)\n",
    "            ]:\n",
    "                if line.startswith(prefix):\n",
    "                    res[stat_name] = float(line[len(prefix):-suff_len])\n",
    "    return res, \"\".join(cmd_out)\n",
    "\n",
    "def run_search(graph_path, ef_ticks=efSearchTicks, n_threads=8, verbose=True):\n",
    "    commands = []\n",
    "    for ef in ef_ticks:\n",
    "        commands.append(search_cmd_template.format(\n",
    "            inputGraph=graph_path,\n",
    "            efSearch=ef,\n",
    "        ))\n",
    "    pool = Pool(processes=n_threads)\n",
    "    results = pool.map(bench_cmd, commands)\n",
    "    output = {\"relevance\": [], \"recall\": [], \"time\": [], \"evals\": []}\n",
    "    for i, (res, cmd_out) in enumerate(results):\n",
    "        if all(key in res for key in output):\n",
    "            for key in output:\n",
    "                output[key].append(res[key])\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"missed result for {} efSearch {}.\".format(graph_path, ef_ticks[i]))\n",
    "                print(commands[i])\n",
    "                print(cmd_out)\n",
    "                \n",
    "    output[\"efSearch\"] = [int(t) for t in ef_ticks]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = OrderedDict()\n",
    "results = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for D in D_RANGE:\n",
    "    label = \"rerank_{}\".format(D)\n",
    "    graph_path = \"data/reranking/rerank_{}.hnsw\".format(D)\n",
    "    build_graph(\n",
    "        DATASET, graph_path,\n",
    "        label, ITEM_COUNT, \"precomputed_ranks\",\n",
    "        recalc=True\n",
    "    )\n",
    "    results[label] = run_search(graph_path)\n",
    "\n",
    "graph_path = \"data/reranking/usual.hnsw\"\n",
    "build_graph(DATASET, graph_path, \"data/items\", 25, \"l2\", recalc=True)\n",
    "results[\"usual\"] = run_search(graph_path)\n",
    "\n",
    "graph_path = \"data/reranking/rerank_greedy.hnsw\"\n",
    "build_graph(DATASET, graph_path, \"rerank_greedy\", ITEM_COUNT, \"precomputed_ranks\", recalc=True)\n",
    "results[\"rerank_greedy\"] = run_search(graph_path)\n",
    "\n",
    "graph_path = \"data/reranking/rerank_trivial.hnsw\"\n",
    "build_graph(DATASET, graph_path, \"rerank_trivial\", ITEM_COUNT, \"precomputed_ranks\", recalc=True)\n",
    "results[\"rerank_trivial\"] = run_search(graph_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_chosen_results(results, keys=None, xlim=None, ylim=None, x_log_scale=False):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.xlabel(\"evals\")\n",
    "    plt.ylabel(\"recall@5\")\n",
    "    if keys is None:\n",
    "        keys = results.keys()\n",
    "    if xlim is not None:\n",
    "        plt.xlim(xlim)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(ylim)\n",
    "    if x_log_scale:\n",
    "        plt.xscale(\"log\")\n",
    "    \n",
    "    for key in keys:\n",
    "        assert key in results\n",
    "        r = results[key]\n",
    "        x = r[\"evals\"]\n",
    "        y = r[\"recall\"]\n",
    "        plt.plot(x, y, label=key)\n",
    "        pt_size = 0.1 if len(x) > 100 else 5\n",
    "        plt.scatter(x, y, s=pt_size)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_chosen_results(\n",
    "    results, x_log_scale=False,\n",
    "    ylim=[0.8, 1], xlim=[100, 1000],\n",
    "    keys=[\n",
    "        \"usual\",\n",
    "        \"rerank_trivial\",\n",
    "#         \"rerank_greedy\",\n",
    "        \"rerank_50\",\n",
    "        \"rerank_400\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
